{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "rvgIcUEneolk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "scaler = MinMaxScaler()\n",
        "input_data = np.array([[3, -1.5, 3, -6.4], [0, 3, -1.3, 4.1], [1, 2.3, -2.9, -4.3]])\n",
        "input_data"
      ],
      "metadata": {
        "id": "YqEmYwdScIph"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.boxplot(input_data)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "INHaqpskdBYb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#warning 메세지를 없앨 때\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "A6V61qDaikOn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_scaled = scaler.fit_transform(input_data)"
      ],
      "metadata": {
        "id": "TPWuFIn5c1Hw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.boxplot(data_scaled)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "QHhsxQxmcImt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "data_scaled = scaler.fit_transform(input_data)\n",
        "plt.boxplot(data_scaled)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "WoCx63K0cIj8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# BREAST CANCER 예제"
      ],
      "metadata": {
        "id": "zKyjHdy5lVaa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터 가져오기\n",
        "from sklearn import datasets\n",
        "cancer = datasets.load_breast_cancer()"
      ],
      "metadata": {
        "id": "igaCPcVacIhU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(cancer.DESCR)"
      ],
      "metadata": {
        "id": "2bCz_O9ncIet"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터 프레임 만들기\n",
        "df = pd.DataFrame(cancer.data)\n",
        "df.columns = cancer.feature_names\n",
        "df['class'] = cancer.target\n",
        "df"
      ],
      "metadata": {
        "id": "kzY8wAczcIcH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 데이터 검사"
      ],
      "metadata": {
        "id": "K1rg6XXvonQA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.head() # 상위 5개 데이터 확인"
      ],
      "metadata": {
        "id": "yi3A2efEcIZV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.shape #데이터의 사이즈 확인"
      ],
      "metadata": {
        "id": "K_so2vFXcIWP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.dtypes #데이터 타입 확인"
      ],
      "metadata": {
        "id": "VJH6OJaxcITl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.isna().sum() #넑값 체크"
      ],
      "metadata": {
        "id": "tpoLFFbPcIRD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.describe() # 기초 통계량 확인"
      ],
      "metadata": {
        "id": "KNoc3y29cIOI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.groupby('class').size() # class의 갯수 확인"
      ],
      "metadata": {
        "id": "vnEMHYhBoJC6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PRELIMINARY ANALYSIS"
      ],
      "metadata": {
        "id": "PrLMXLZoosvW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 단변량 분석 (히스토그램, 박스플랏, 카운트 플랏)"
      ],
      "metadata": {
        "id": "Eso0Vms6ojvB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.hist(figsize=(15,15))\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "dFRtA6DSojoD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.boxplot() #scaling이 필요함"
      ],
      "metadata": {
        "id": "-3tr7EQyqT-b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 다변량 분석 (산점도, 상관관계)\n",
        "import seaborn as sns\n",
        "sns.pairplot(df, hue='class')"
      ],
      "metadata": {
        "id": "dFqh8b3Qojh6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.corr()"
      ],
      "metadata": {
        "id": "5KK7p-qKvfli"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10,10))\n",
        "sns.heatmap(df.corr(), annot=True)"
      ],
      "metadata": {
        "id": "usxvVA9yojcp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 모델링"
      ],
      "metadata": {
        "id": "62Om-j7rtE1S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# x and y split\n",
        "y = df['class']\n",
        "x = df.drop('class', axis=1)"
      ],
      "metadata": {
        "id": "B0njNZybojZZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#scaling\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "scaler = MinMaxScaler()\n",
        "x_scaled = scaler.fit_transform(x)\n",
        "plt.boxplot(x_scaled)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "KNJoIFiCojPB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# resampling\n",
        "from imblearn.over_sampling import SMOTE\n",
        "smote = SMOTE()\n",
        "x_smote, y_smote = smote.fit_resample(x_scaled, y)\n",
        "sns.countplot(x=y_smote)"
      ],
      "metadata": {
        "id": "mMzl5Ta5tblh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train and test split\n",
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_test, y_train, y_test = train_test_split(x_smote, y_smote, test_size=.2, random_state=1)"
      ],
      "metadata": {
        "id": "HLfbulssts1p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6YD7E6eWtsyS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SCALING"
      ],
      "metadata": {
        "id": "gO9FhdRbFNV1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# SCALING\n",
        "import warnings\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#정규화\n",
        "data = np.array([[3, -1.5, 3, -6.4],\n",
        "                 [0, 3, -1.3, 4.1],\n",
        "                 [1, 2.3, -2.9, -4.3]])\n",
        "plt.boxplot(data)\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "MtZZf2sbFTP0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "scaler = MinMaxScaler()\n",
        "scaled_data = scaler.fit_transform(data)\n",
        "plt.boxplot(scaled_data)\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "icqXQ3qwT0mc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#표준화\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "scaled_data = scaler.fit_transform(data)\n",
        "plt.boxplot(scaled_data)\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "c3rmRPkiT201"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import scale #standard scaler와 같은 결과\n",
        "data_scaled = scale(data)\n",
        "plt.boxplot(data_scaled)\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "IX6d_THJT7Ky"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import normalize\n",
        "data_normalized = normalize(data, norm='l2') #l1 uses absolute value, l2 uses square root\n",
        "plt.boxplot(data_normalized)\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "wM26-d7nUGDE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import Normalizer\n",
        "scaler = Normalizer()\n",
        "data_normalized = scaler.fit_transform(data)\n",
        "plt.boxplot(data_normalized)\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "tcuahxLpVB5E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import Binarizer\n",
        "data_scaled = Binarizer(threshold=1.4).transform(data)\n",
        "plt.boxplot(data_scaled)\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "T8dU9nhoFQjN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn import datasets\n",
        "iris = datasets.load_iris()"
      ],
      "metadata": {
        "id": "pRTV1riydUKd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y = iris.target\n",
        "x = iris.data"
      ],
      "metadata": {
        "id": "ZuA2E-X7dvLq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=.2, random_state=1)"
      ],
      "metadata": {
        "id": "Dq5ixZJ6d2Yu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "rf = RandomForestClassifier()\n",
        "rf.fit(x_train, y_train)\n",
        "rf.score(x_test, y_test)"
      ],
      "metadata": {
        "id": "JH6NhDwTeHaG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "print(classification_report(y_test, rf.predict(x_test)))"
      ],
      "metadata": {
        "id": "zqffyS1Bf3aW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "params = { 'n_estimators' : [10, 100],\n",
        "           'max_depth' : [6, 8, 10, 12],\n",
        "           'min_samples_leaf' : [8, 12, 18],\n",
        "           'min_samples_split' : [8, 16, 20]\n",
        "            }"
      ],
      "metadata": {
        "id": "p5BHCZCAeex2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# GridSearchCV 수행\n",
        "rf = RandomForestClassifier(random_state = 0, n_jobs = -1)\n",
        "grid_cv = GridSearchCV(rf, param_grid = params, cv = 3, n_jobs = -1)\n",
        "grid_cv.fit(x_train, y_train)"
      ],
      "metadata": {
        "id": "sxFGuyVbfB5V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('최적 하이퍼 파라미터: ', grid_cv.best_params_)\n",
        "print('최고 예측 정확도: {:.4f}'.format(grid_cv.best_score_))"
      ],
      "metadata": {
        "id": "xdzudFnWeW9F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "grid_cv.best_estimator_"
      ],
      "metadata": {
        "id": "ww1AWIP1fmBe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(classification_report(y_test, grid_cv.predict(x_test)))"
      ],
      "metadata": {
        "id": "qVNQ5nUHgOZG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Breast Cancer 예제"
      ],
      "metadata": {
        "id": "axAN-K-ODfNe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn import datasets"
      ],
      "metadata": {
        "id": "9Ps7N8ZODXni"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터 프레임 만들기\n",
        "cancer = datasets.load_breast_cancer()\n",
        "df = pd.DataFrame(cancer.data, columns=cancer.feature_names)\n",
        "df['class'] = cancer.target"
      ],
      "metadata": {
        "id": "Hi4UN0uEDZq5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(cancer.DESCR)"
      ],
      "metadata": {
        "id": "M_j1UYod8yYL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#데이터 검사\n",
        "df.head()\n",
        "df.shape\n",
        "df.dtypes\n",
        "df.isna().sum()\n",
        "df.describe()"
      ],
      "metadata": {
        "id": "TY5Lvb_pDa6T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#preliminary analysis\n",
        "df.hist(figsize=(10,10))\n",
        "plt.tight_layout()"
      ],
      "metadata": {
        "id": "RB2rp5kFDdOr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.boxplot(figsize=(10,10)) #scaling 필요\n",
        "plt.xticks(rotation=60)\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "L8CB1g77EKJ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "sns.pairplot(df, hue='class')"
      ],
      "metadata": {
        "id": "1aMGg-QcEiyJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10,10))\n",
        "sns.heatmap(df.corr(), annot=True)"
      ],
      "metadata": {
        "id": "JqrNitpyEkdh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.heatmap(df.corr()[df.corr()>=.7])"
      ],
      "metadata": {
        "id": "9j92SMQNE0lQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#x and y split\n",
        "y = df['class'] #종속변수\n",
        "x = df.drop('class', axis=1) #독립변수\n",
        "# x = df[col_lst]"
      ],
      "metadata": {
        "id": "sq1QTpL5EyL4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#scaling x\n",
        "x.boxplot()\n",
        "plt.xticks(rotation=60)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "7cuJxKHvE6LZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "scaler = MinMaxScaler()\n",
        "x_scaled = scaler.fit_transform(x)\n",
        "plt.boxplot(x_scaled)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "2CSSYfTLE8kk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# resample\n",
        "sns.countplot(x=df['class'])"
      ],
      "metadata": {
        "id": "V92BK6BFWXhw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install imblearn\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "rus = RandomUnderSampler()\n",
        "x_rus, y_rus = rus.fit_resample(x_scaled, y)\n",
        "sns.countplot(x=y_rus)"
      ],
      "metadata": {
        "id": "FgfrqgUOW0kc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#train and test split\n",
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_test, y_train, y_test = train_test_split(x_rus, y_rus, test_size=.3, random_state=1)"
      ],
      "metadata": {
        "id": "jPanqlDlXpHb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import cross_val_score"
      ],
      "metadata": {
        "id": "98XiC09TYJP7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_lst = [LogisticRegression(), DecisionTreeClassifier(), RandomForestClassifier(), LinearDiscriminantAnalysis(), KNeighborsClassifier(), SVC()]\n",
        "score_lst = []\n",
        "cv_lst = []\n",
        "for model in model_lst:\n",
        "    model.fit(x_train, y_train)\n",
        "    score_lst.append(model.score(x_test, y_test))\n",
        "    cv_lst.append(cross_val_score(model, x_train, y_train, cv=10, scoring='accuracy'))"
      ],
      "metadata": {
        "id": "oKBVoEWRZxof"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "acc_df = pd.DataFrame(score_lst, columns=['accuracy'], index=['LOG', 'DT', 'RF', 'LDA', 'KNN', 'SVM'])\n",
        "acc_df"
      ],
      "metadata": {
        "id": "tUUqAAH3afBs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cv_df = pd.DataFrame(cv_lst)\n",
        "cv_df = cv_df.T\n",
        "cv_df.columns = ['LOG', 'DT', 'RF', 'LDA', 'KNN', 'SVM']\n",
        "cv_df"
      ],
      "metadata": {
        "id": "MSCNI8PCalgL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cv_df.boxplot()"
      ],
      "metadata": {
        "id": "4QwBHIk9b2OC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "means = cv_df.mean()\n",
        "stds = cv_df.std()\n",
        "cv_summary = pd.concat([means, stds], axis=1)\n",
        "cv_summary.columns = ['MEAN', 'STD']\n",
        "cv_summary"
      ],
      "metadata": {
        "id": "V8AUDF7rbe2x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# SVM\n",
        "svm = SVC()\n",
        "svm.fit(x_train, y_train)"
      ],
      "metadata": {
        "id": "xqBum52yiPwA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = svm.predict(x_test)\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "id": "6AbdZ7WJikWc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 파라미터튜닝\n",
        "# https://www.geeksforgeeks.org/svm-hyperparameter-tuning-using-gridsearchcv-ml/\n",
        "param_grid = {'C' : [.1, 1, 10, 100, 1000],\n",
        "              'gamma': [1, .1, .01, .001, .0001],\n",
        "              'kernel': ['rbf']}"
      ],
      "metadata": {
        "id": "2PxW219WhN3n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "grid_cv = GridSearchCV(svm, param_grid, refit=True, verbose=3)\n",
        "grid_cv.fit(x_train, y_train)"
      ],
      "metadata": {
        "id": "RSqNhMXTh1l1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "grid_cv.best_params_"
      ],
      "metadata": {
        "id": "9F3jx-UriGvE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "grid_cv.best_estimator_"
      ],
      "metadata": {
        "id": "h86CfQhYiLct"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = grid_cv.predict(x_test)\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "id": "4Tml8DVRiQXc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_data = df.iloc[0,:-1]\n",
        "grid_cv.predict(scaler.fit_transform(test_data.values.reshape(1,-1))) #양성이라고 예측"
      ],
      "metadata": {
        "id": "bRJ4LP0mhM4d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "# , plot_roc_curve\n",
        "sns.heatmap(confusion_matrix(y_test, grid_cv.predict(x_test)), annot=True)"
      ],
      "metadata": {
        "id": "OqCn3Tq7rY_w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(classification_report(y_test, grid_cv.predict(x_test)))"
      ],
      "metadata": {
        "id": "ekgWunl6sDZ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# https://scikit-learn.org/1.0/modules/generated/sklearn.metrics.plot_roc_curve.html\n",
        "from sklearn.metrics import roc_curve, roc_auc_score\n",
        "fpr, tpr, _ = roc_curve(y_test, grid_cv.predict(x_test))\n",
        "auc = roc_auc_score(y_test, grid_cv.predict(x_test))"
      ],
      "metadata": {
        "id": "tcdLdJnGjUUX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(fpr,tpr,label=\"AUC=\"+str(auc))\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.legend(loc=4)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "gIVkx3RlkhLs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#변수 중요도 ==> 변수 줄일 때 사용\n",
        "rf_model = RandomForestClassifier()\n",
        "rf_model.fit(x_train, y_train)\n",
        "rf_model.feature_importances_ #중요한 변수에 대한 점수\n",
        "sns.barplot(y=df.columns[:-1], x=rf_model.feature_importances_)\n",
        "plt.show()\n",
        "col_lst = df.columns[:-1][rf_model.feature_importances_ >= .06] #중요도가 높은 변수들\n",
        "col_lst"
      ],
      "metadata": {
        "id": "Up7oQhaPfgCR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn import datasets"
      ],
      "metadata": {
        "id": "ZjPITYoy081g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Boson data. 보스톤 데이터 로딩\n",
        "from sklearn.datasets import fetch_openml\n",
        "housing = fetch_openml(name=\"house_prices\", as_frame=True)"
      ],
      "metadata": {
        "id": "hYA1DDol241y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(housing)"
      ],
      "metadata": {
        "id": "jAt7Mmcx3Ay0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "housing.keys()"
      ],
      "metadata": {
        "id": "MMgqjfPp4Dhs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "housing.feature_names"
      ],
      "metadata": {
        "id": "ITINYTdm5S4s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('/content/boston.csv', delimiter=r\"\\s+\")\n",
        "df.columns = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B',' LSTAT', 'MEDV']\n",
        "df"
      ],
      "metadata": {
        "id": "-I7jU4co4lGN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Data exploration (head, shape, dtyes, describe, isna.sum()). 데이터 탐색\n",
        "df.head()\n",
        "df.shape\n",
        "df.dtypes\n",
        "df.describe()\n",
        "df.isna().sum()"
      ],
      "metadata": {
        "id": "z422R9d24qKM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns"
      ],
      "metadata": {
        "id": "-BkcHDwk4uXa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#시각화\n",
        "df.hist()\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "FaMWiQ_z65mj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.boxplot() #scaling\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Th4q3ZB_67ab"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.pairplot(df)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "L1iQfk5S6-Kq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10,10))\n",
        "sns.heatmap(df.corr(), annot=True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "IzOt8wwF6_zL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# X and y split. 독립, 종속변수 분리\n",
        "y = df.MEDV\n",
        "x = df.drop('MEDV', axis=1)"
      ],
      "metadata": {
        "id": "Ec12aBya7Cv6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Data preprocessing (StandardScaler). 전처리\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "x_scaled = scaler.fit_transform(x)\n",
        "plt.boxplot(x_scaled)\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "W3MOevQS7Ej7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train and test split (sklearn.model_selection.train_test_split, test size=.2). 훈련, 테스트 데이터 분리\n",
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_test, y_train, y_test = train_test_split(x_scaled, y, test_size=.2, random_state=1)"
      ],
      "metadata": {
        "id": "4yo7D06m7NCy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Linear Regression 회귀분석\n",
        "from sklearn.linear_model import LinearRegression\n",
        "lin_model = LinearRegression()\n",
        "lin_model.fit(x_train, y_train)\n",
        "lin_model.coef_ #기울기\n",
        "lin_model.intercept_ #y절편"
      ],
      "metadata": {
        "id": "TF_AQweG7PIy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model evaluation (r2_score) 모델평가\n",
        "lin_model.score(x_test, y_test)\n",
        "from sklearn.metrics import r2_score\n",
        "r2_score(y_test, lin_model.predict(x_test)) #0.7545577207950153"
      ],
      "metadata": {
        "id": "5BF5kudO7Rea"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 10 fold cross validation (r2) 10겹 교차검증\n",
        "from sklearn.model_selection import cross_val_score\n",
        "lin_cv = cross_val_score(lin_model, x_test, y_test, scoring='r2', cv=10)"
      ],
      "metadata": {
        "id": "EOjTxduj7TAF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Decision Tree Regressor 의사결정 트리\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "dtr = DecisionTreeRegressor()\n",
        "dtr.fit(x_train, y_train)"
      ],
      "metadata": {
        "id": "Lqt6Ww0d7UpD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model evaluation (r2_score) 모델평가\n",
        "dtr.score(x_test, y_test) # 0.8274774714959514\n",
        "r2_score(y_test, dtr.predict(x_test))"
      ],
      "metadata": {
        "id": "6SH3R9jM7W0H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 10 fold cross validation 10겹 교차검증\n",
        "dtr_cv = cross_val_score(dtr, x_test, y_test, scoring='r2', cv=10)"
      ],
      "metadata": {
        "id": "uShX10qk7YUv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Random Forest Regressor 랜덤 포레스트\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "rfr = RandomForestRegressor()\n",
        "rfr.fit(x_train, y_train)"
      ],
      "metadata": {
        "id": "k-2Tyory7Z3U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model evaluation (r2_score) 모델평가\n",
        "rfr.score(x_test, y_test) #0.9059660350016387\n",
        "r2_score(y_test, rfr.predict(x_test))"
      ],
      "metadata": {
        "id": "w3tmonHK7bTz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 10 fold cross validation 10겹 교차검증\n",
        "rfr_cv = cross_val_score(rfr, x_test, y_test, scoring='r2', cv=10)"
      ],
      "metadata": {
        "id": "DQJcy3Zm7cny"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model Comparison 모델 비교\n",
        "# Mean and standard deviation table 평균, 표준편차 테이블\n",
        "df_com = pd.DataFrame({\n",
        "    'mean': [lin_cv.mean(), dtr_cv.mean(), rfr_cv.mean()],\n",
        "    'std':[lin_cv.std(), dtr_cv.std(), rfr_cv.std()]\n",
        "    }, index=['LR', 'DT', 'RF'])\n",
        "df_com #RF의 정확도가 가장 높음"
      ],
      "metadata": {
        "id": "Wheg7nZ17cxd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Boxplot 박스플랏\n",
        "plt.boxplot([lin_cv, dtr_cv, rfr_cv])\n",
        "plt.xticks([1,2,3], ['LR', 'DT', 'RF'])\n",
        "plt.show() #RF의 정확도가 가장 높음"
      ],
      "metadata": {
        "id": "GKa6uuva8Haq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#pima indian diabete dataset\n",
        "pima = pd.read_csv('/content/pima-indians-diabetes.csv', header=None)\n",
        "pima.columns = ['preg', 'plas','pres', 'skin', 'test', 'mass', 'pedi', 'age',  'class']"
      ],
      "metadata": {
        "id": "eTDbd01n8dta"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# X and y split. 독립, 종속변수 분리\n",
        "y = pima['class']\n",
        "x = pima.drop('class', axis=1)\n",
        "x.boxplot()"
      ],
      "metadata": {
        "id": "-VHmbA9W8wb3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "scaler = MinMaxScaler()\n",
        "x_scaled = scaler.fit_transform(x)\n",
        "plt.boxplot(x_scaled)\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "uLV2_EQU83AP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Splitting Dataset (decide the test size). 테스트셋 크기를 결정해서 데이터셋을 분할\n",
        "x_train, x_test, y_train, y_test = train_test_split(x_scaled, y, test_size=.2, random_state=1)\n",
        "\n",
        "# Resampling using SMOTE. 리샘플링\n",
        "sns.countplot(y_train)\n",
        "from imblearn.over_sampling import SMOTE\n",
        "smote = SMOTE()\n",
        "x_smote, y_smote = smote.fit_resample(x_train, y_train)\n",
        "\n",
        "# Build DT, RF, LDA, kNN, and SVM models. 5개 모델 구축\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "models = [DecisionTreeClassifier(),RandomForestClassifier(),LinearDiscriminantAnalysis(),KNeighborsClassifier(),SVC()]\n",
        "cv_list = []\n",
        "acc_list = []\n",
        "for model in models:\n",
        "    model.fit(x_train, y_train) #학습\n",
        "    cv_list.append(cross_val_score(model, x_test, y_test, cv=10, scoring='accuracy'))\n",
        "    acc_list.append(model.score(x_test, y_test))\n",
        "\n",
        "# Use accuracy score to compare the models. 정확도를 사용하여 평가\n",
        "model_columns = ['DT', 'RF', 'LDA', 'KNN', 'SVM']\n",
        "df_com = pd.DataFrame(acc_list, index=model_columns)\n",
        "df_com #RF, KNN, SVM이 정확도가 높음\n",
        "\n",
        "# Use 10-fold cross validation to compare the models. 교차검증을 사용하여 모델 비교\n",
        "df_cv = pd.DataFrame(cv_list, index=model_columns)\n",
        "df_summary = pd.concat([df_cv.mean(axis=1),df_cv.std(axis=1)], axis=1)\n",
        "df_summary.columns = ['MEAN', 'STD']\n",
        "df_summary #SVM 정확도가 제일 높음\n",
        "\n",
        "# Use confusion matrix, ROC curve to compare the models. 혼동행렬, 락커브를 이용해서 모델 비교\n",
        "svm = SVC()\n",
        "svm.fit(x_train, y_train)\n",
        "\n",
        "from sklearn.metrics import confusion_matrix, plot_roc_curve\n",
        "cfm = confusion_matrix(y_test, svm.predict(x_test))\n",
        "sns.heatmap(cfm, annot=True)\n",
        "\n",
        "plot_roc_curve(svm, x_test, y_test)\n",
        "\n",
        "# Predict with actual data. 실데이터를 가지고 예측하기\n",
        "test_data = pima.iloc[0,:-1].values.reshape(1,-1)\n",
        "test_data = scaler.transform(test_data)\n",
        "svm.predict(test_data) #양성이라고 예측"
      ],
      "metadata": {
        "id": "O2BN3la20MKr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#voting classifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "log_model = LogisticRegression() #0.935672514619883\n",
        "rf_model = RandomForestClassifier() #0.9415204678362573\n",
        "svm_model = SVC() #0.8713450292397661\n",
        "voting_model = VotingClassifier(estimators=[('log_model', log_model),\n",
        "                                            ('rf_model', rf_model),\n",
        "                                            ('svm_model', svm_model)], voting='hard')\n",
        "voting_model.fit(x_smote, y_smote)\n",
        "print('{}: {}'.format(voting_model.__class__.__name__, voting_model.score(x_test, y_test))) #0.935672514619883\n",
        "\n",
        "# 모델 비교\n",
        "for model in (log_model, rf_model, svm_model):\n",
        "    model.fit(x_smote, y_smote)\n",
        "    print('{}: {}'.format(model.__class__.__name__, model.score(x_test, y_test)))\n",
        "\n",
        "#random forest\n",
        "rf_model = RandomForestClassifier()\n",
        "print(rf_model.n_estimators) #100\n",
        "rf_model.fit(x_smote, y_smote)\n",
        "print(rf_model.score(x_test, y_test)) #0.935672514619883\n",
        "rf_model.feature_importances_ #중요한 변수에 대한 점수\n",
        "\n",
        "#변수 중요도 ==> 변수 줄일 때 사용\n",
        "rf_model.feature_importances_ #중요한 변수에 대한 점수\n",
        "ranking = pd.Series(rf_model.feature_importances_, index=x.columns)\n",
        "ranking = ranking.sort_values(ascending=False)\n",
        "sns.barplot(y=ranking.index, x=ranking)\n",
        "plt.show()\n",
        "x.shape\n",
        "\n",
        "#PARAMETER TUNING\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "params = {\n",
        "    'n_estimators': [100],\n",
        "    'max_depth': [6, 8, 10, 12],\n",
        "    'min_samples_leaf': [8, 12, 18],\n",
        "    'min_samples_split': [8, 16, 20]}\n",
        "rf = RandomForestClassifier(random_state=0, n_jobs=-1)\n",
        "grid_cv = GridSearchCV(rf, param_grid=params, cv=2, n_jobs=-1)\n",
        "grid_cv.fit(x_train, y_train)\n",
        "print(grid_cv.best_params_) #best hyper parameter\n",
        "print(grid_cv.best_score_) #best score 0.9271356783919598\n",
        "\n",
        "rf2 = RandomForestClassifier(random_state=0, n_jobs=1, max_depth=6, min_samples_leaf=8, min_samples_split=8, n_estimators=100)\n",
        "rf2.fit(x_train, y_train)\n",
        "rf2.score(x_test, y_test) #0.9298245614035088\n",
        "\n",
        "#ADA BOOSTING\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "# 아다부스트 모델 구축\n",
        "ada_model = AdaBoostClassifier(\n",
        "    DecisionTreeClassifier(max_depth=1),\n",
        "    n_estimators = 200,\n",
        "    algorithm = 'SAMME.R',\n",
        "    learning_rate=0.5\n",
        ")\n",
        "\n",
        "# 모델 학습\n",
        "ada_model.fit(x_train, y_train)\n",
        "ada_model.score(x_test, y_test) #0.9649122807017544\n",
        "\n",
        "#Gradient Boosting\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "import time\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "gb_model = GradientBoostingClassifier(random_state=0)\n",
        "gb_model.fit(x_train,y_train)\n",
        "gb_pred = gb_model.predict(x_test)\n",
        "gb_accuracy = accuracy_score(y_test, gb_pred)\n",
        "\n",
        "print(f'GBM 정확도: {np.round(gb_accuracy, 4)}') #0.9385964912280702\n",
        "print(f'GBM 수행 시간: {np.round((time.time() - start_time), 1)} 초')\n",
        "\n",
        "#PARAMETER TUNING\n",
        "params = {\n",
        "    'n_estimators': [100, 500],\n",
        "    'learning_rate': [.05, .1]}\n",
        "grid_cv = GridSearchCV(gb_model, param_grid=params, cv=2, verbose=1, n_jobs=-1)\n",
        "grid_cv.fit(x_train, y_train)\n",
        "print('최적 하이퍼 파라미터: \\n', grid_cv.best_params_)\n",
        "print('최고 예측 정확도: {0:.4f}'.format(grid_cv.best_score_))\n",
        "\n",
        "# GridSearchCV를 이용해 최적으로 학습된 estimators로 예측 수행\n",
        "gb_pred = grid_cv.best_estimator_.predict(x_test)\n",
        "gb_accuracy = accuracy_score(y_test, gb_pred)\n",
        "print('GBM 정확도: {0:.4f}'.format(gb_accuracy)) #0.9649\n",
        "\n",
        "#XGBOOSTING\n",
        "# https://velog.io/@fiifa92/%EC%95%99%EC%83%81%EB%B8%94Ensemble-%EA%B8%B0%EB%B2%95#:~:text=%EC%95%99%EC%83%81%EB%B8%94%EC%9D%80%20%EC%97%AC%EB%9F%AC%20%EA%B0%9C%EC%9D%98%20%EB%B6%84%EB%A5%98%EA%B8%B0,%EB%8B%A4%EC%96%91%ED%95%9C%20%EC%95%99%EC%83%81%EB%B8%94%20%EB%B0%A9%EC%8B%9D%EC%9D%B4%20%EC%9E%88%EB%8B%A4.\n",
        "# !pip install xgboost\n",
        "from xgboost import XGBClassifier\n",
        "xgb_wrapper = XGBClassifier(n_estimators=400, learning_rate=0.1, max_depth=3)\n",
        "xgb_wrapper.fit(x_train, y_train)\n",
        "print(xgb_wrapper.score(x_test, y_test)) #0.9473684210526315\n",
        "w_preds = xgb_wrapper.predict(x_test)\n",
        "w_pred_proba = xgb_wrapper.predict_proba(x_test)[:, 1]\n",
        "\n",
        "#조기중단 100\n",
        "xgb_wrapper.fit(x_train, y_train, early_stopping_rounds=100, eval_metric=\"logloss\", eval_set=[(x_test, y_test)], verbose=True)\n",
        "print(xgb_wrapper.score(x_test, y_test)) #0.9532163742690059\n",
        "ws100_preds = xgb_wrapper.predict(x_test)\n",
        "ws100_pred_proba = xgb_wrapper.predict_proba(x_test)[:, 1]\n",
        "\n",
        "#조기중단 10\n",
        "xgb_wrapper.fit(x_train, y_train, early_stopping_rounds=10, eval_metric=\"logloss\", eval_set=[(x_test, y_test)], verbose=True)\n",
        "print(xgb_wrapper.score(x_test, y_test)) #0.9532163742690059\n",
        "ws10_preds = xgb_wrapper.predict(x_test)\n",
        "ws10_pred_proba = xgb_wrapper.predict_proba(x_test)[:, 1]\n",
        "\n",
        "from xgboost import plot_importance\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(10, 12))\n",
        "plot_importance(xgb_wrapper, ax=ax)\n",
        "\n",
        "#LIGHT GBM\n",
        "!pip install lightgbm\n",
        "from lightgbm import LGBMClassifier, plot_importance\n",
        "\n",
        "lgbm_model = LGBMClassifier(n_estimators=400)\n",
        "lgbm_model.fit(x_train, y_train, early_stopping_rounds=100, eval_metric='logloss', eval_set = [(x_test, y_test)], verbose=True)\n",
        "lgbm_model.score(x_test, y_test) #0.9473684210526315\n",
        "\n",
        "plot_importance(lgbm_model, max_num_features=15)"
      ],
      "metadata": {
        "id": "BWyHpdBGDVRk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import os\n",
        "os.chdir('E:/Data')\n",
        "\n",
        "# Import the train data of Loan Predication data set, do some basic exploration tasks, and use the graphs to visualize the data. 대출 훈련데이터를 가져와서 기본 탐색을 수행하고 시각화 하시오.\n",
        "loan = pd.read_csv('loan.csv')\n",
        "loan.columns\n",
        "# loan.set_index('Loan_ID', inplace=True) #인덱스를 바꿀때 쓰는 코드\n",
        "\n",
        "# head(), shape, dtypes, describe(), isna.sum()\n",
        "loan.head()\n",
        "loan.shape\n",
        "loan.dtypes #분류알고리즘을 써야함\n",
        "a = loan.describe()\n",
        "loan.isna().sum() #널값처리가 필요함\n",
        "\n",
        "# Histogram, boxplot, pairplot, heatmap, countplot\n",
        "loan.hist()\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "loan.boxplot() #스케일링이 필요함\n",
        "plt.show()\n",
        "\n",
        "loan.columns\n",
        "sns.countplot(loan.Loan_Status) #리샘플링이 필요함\n",
        "plt.show()\n",
        "\n",
        "sns.pairplot(loan, hue='Loan_Status')\n",
        "plt.show()\n",
        "\n",
        "sns.heatmap(loan.corr(), annot=True)\n",
        "plt.show()\n",
        "\n",
        "#데이터 탐색 및 시각화\n",
        "#여러개의 그래프를 한군데에 그릴 때\n",
        "loan.columns\n",
        "fig = plt.figure(figsize=(12, 8))\n",
        "fig.add_subplot(221)   #top left\n",
        "plt.boxplot(loan.ApplicantIncome)\n",
        "plt.title('Applicant Income')\n",
        "fig.add_subplot(222)   #top right\n",
        "plt.hist(loan.ApplicantIncome, bins=20)\n",
        "plt.title('Applicant Income')\n",
        "fig.add_subplot(223)   #bottom left\n",
        "plt.boxplot(loan.CoapplicantIncome)\n",
        "plt.title('Coapplicant Income')\n",
        "fig.add_subplot(224)   #bottom right\n",
        "plt.hist(loan.CoapplicantIncome, bins=20)\n",
        "plt.title('Coapplicant Income')\n",
        "plt.show()\n",
        "\n",
        "#계량형 데이터를 범주형으로 비교할 때\n",
        "loan.boxplot(column='ApplicantIncome', by='Education')\n",
        "plt.show()\n",
        "\n",
        "#범주형 데이터 빈도테이블\n",
        "loan.Property_Area.value_counts()\n",
        "loan.Credit_History.value_counts(dropna=False)\n",
        "\n",
        "#두개 범주형 변수의 빈도테이블\n",
        "a = pd.crosstab(loan.Loan_Status, loan.Credit_History)\n",
        "a.plot(kind='bar')\n",
        "\n",
        "sns.heatmap(a, annot=True, fmt='g')\n",
        "\n",
        "#세개 범주형 변수의 빈도테이블\n",
        "a = pd.crosstab(loan.Loan_Status, [loan.Credit_History, loan.Gender])\n",
        "a.plot(kind='bar', stacked=True)\n",
        "\n",
        "#계량형 데이터를 범주형 데이터로 변환하여 빈도테이블\n",
        "loan.LoanAmount.hist(bins=30)\n",
        "\n",
        "labels = [\"low\",\"medium\",\"high\",\"very high\"]\n",
        "cut_points = [90,140,190]\n",
        "minval = loan.LoanAmount.min()\n",
        "maxval = loan.LoanAmount.max()\n",
        "break_points = [minval] + cut_points + [maxval]\n",
        "loan[\"LoanAmount_Bin\"] = pd.cut(loan.LoanAmount, bins=break_points, labels=labels, include_lowest=True)\n",
        "loan.LoanAmount_Bin.value_counts(sort=False, dropna=False)\n",
        "\n",
        "# Filling missing data 분실값 처리\n",
        "loan.isna().sum()\n",
        "# Mean for LoanAmount 대출액은 평균으로 대체\n",
        "loan.LoanAmount.hist()\n",
        "loan.LoanAmount.fillna(loan.LoanAmount.mean(), inplace=True)\n",
        "\n",
        "# Mode for Self_Employed, Gender, Married, Dependents, Loan_Amount_Term, and Credit_History 나머지는 최빈수로 대체\n",
        "loan.Self_Employed.fillna(loan.Self_Employed.mode()[0], inplace=True)\n",
        "\n",
        "for i in ['Gender', 'Married', 'Dependents', 'Loan_Amount_Term', 'Credit_History']:\n",
        "    loan[i].fillna(loan[i].mode()[0], inplace=True)\n",
        "\n",
        "# Encoding non-numeric values. 범주형 변수를 인코딩\n",
        "loan.dtypes\n",
        "# Encode using get_dummies --> Property_Area\n",
        "dummy = pd.get_dummies(loan.Property_Area)\n",
        "loan = pd.concat([loan, dummy], axis=1)\n",
        "\n",
        "# Encode using sklearn label encoder --> 나머지 변수들\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "le = LabelEncoder()\n",
        "loan.Gender = le.fit_transform(loan.Gender)\n",
        "le.inverse_transform(loan.Gender) #숫자를 글짜로 복귀\n",
        "\n",
        "# Encode multiple variables using for loop\n",
        "for i in ['Married','Dependents','Education','Self_Employed','Loan_Status']:\n",
        "    le = LabelEncoder()\n",
        "    loan[i] = le.fit_transform(loan[i])\n",
        "\n",
        "# X and y split 독립변수, 종속변수 분리\n",
        "# Loan_Status for dependent variables\n",
        "# Other variables for independent variables\n",
        "loan.columns\n",
        "y = loan.Loan_Status\n",
        "x = loan.drop(['Loan_ID', 'Property_Area', 'LoanAmount_Bin', 'Loan_Status'], axis=1)\n",
        "\n",
        "# X값 스케일링(MinMaxScaler, StandardScaler)\n",
        "x.boxplot()\n",
        "plt.show()\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "x = scaler.fit_transform(x)\n",
        "\n",
        "plt.boxplot(x)\n",
        "plt.show()\n",
        "\n",
        "# Train and test split 훈련, 테스트데이터 분리\n",
        "# From sklearn.model_selection import train_test_split\n",
        "# X_train, x_test, y_train, y_test = train_test_split(x, y, test_size=.2, random_state=7) #데이터를 나눠서 80프로는 훈련용으로 20프로는 테스트용으로 사용함\n",
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=.2, random_state=1)\n",
        "\n",
        "# 리샘플링 (오버샘플링, 언더샘플링, SMOTE)\n",
        "sns.countplot(y_train)\n",
        "plt.show()\n",
        "\n",
        "from imblearn.over_sampling import SMOTE\n",
        "smote = SMOTE()\n",
        "x_train, y_train = smote.fit_resample(x_train, y_train)\n",
        "\n",
        "sns.countplot(y_train)\n",
        "plt.show()\n",
        "\n",
        "# Build a logistic regression model. 로지스틱 회귀모델 구축\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "model = LogisticRegression()\n",
        "model.fit(x_train, y_train)\n",
        "\n",
        "# Predict the loan status and check the accuracy score of the model with cross validation. 예측하고 정확도로 교차검증\n",
        "model.score(x_test, y_test) #0.7642276422764228\n",
        "\n",
        "from sklearn.model_selection import cross_val_score\n",
        "cv_score = cross_val_score(model, x_test, y_test, cv=10, scoring='accuracy')\n",
        "cv_score.mean()\n",
        "cv_score.std()\n",
        "\n",
        "# Create a confusion matrix,  classification report, and ROC curve. 혼동행렬, 분류보고서, ROC커브 생성\n",
        "from sklearn.metrics import confusion_matrix, classification_report, plot_roc_curve\n",
        "y_pred = model.predict(x_test)\n",
        "confusion_matrix(y_test, y_pred)\n",
        "print(classification_report(y_test, y_pred))\n",
        "plot_roc_curve(model, x_test, y_test)"
      ],
      "metadata": {
        "id": "K18fozve74Jv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jW6zuSKa1sGT"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "#문서 인코딩\n",
        "text = ['This is the first document.',\n",
        "          'This document is the second document.',\n",
        "          'And this is the third one.',\n",
        "          'Is this the first document?']\n",
        "\n",
        "#COUNT VECTORIZER\n",
        "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
        "cv = CountVectorizer()\n",
        "df_x = pd.DataFrame(cv.fit_transform(text).toarray()) #숫자화시켜서 데이터프레임 생성\n",
        "df_x.columns = cv.get_feature_names() #단어이름을 컬럼이름으로!\n",
        "df_x\n",
        "\n",
        "#N-GRAM\n",
        "cv = CountVectorizer(analyzer='word', ngram_range=(2,2)) #두개의 단어를 기준으로 숫자화\n",
        "df_x2 = pd.DataFrame(cv.fit_transform(text).toarray()) #숫자화시킨 데이터 프레임\n",
        "df_x2.columns = cv.get_feature_names() #단어이름 컬럼\n",
        "df_x2\n",
        "\n",
        "#TFIDF VECTORIZER\n",
        "tfidf = TfidfVectorizer()\n",
        "df_x3 = pd.DataFrame(tfidf.fit_transform(text).toarray())\n",
        "df_x3.columns = tfidf.get_feature_names()\n",
        "df_x3\n",
        "\n",
        "#REGULAR EXPRESSION TOKENIZER\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "sentence  = \"Think and wonder, wonder and think.\"\n",
        "token = RegexpTokenizer(r\"\\w+\") #한개 이상의 글자\n",
        "words = token.tokenize(sentence)\n",
        "# 비슷한 방법\n",
        "# import re\n",
        "# words = re.split('\\W+', sentence) #글자가 아닌 것으로 나눔\n",
        "print(words)\n",
        "\n",
        "#COUNT VECTORIZER PARAMETERS\n",
        "text = ['This is the first document.',\n",
        "          'This document is the second document.',\n",
        "          'And this is the third one.',\n",
        "          'Is this the first document?']\n",
        "token = RegexpTokenizer(r'[a-zA-Z0-9]+') #알파벳, 숫자\n",
        "cv3 = CountVectorizer(lowercase=True,#소문자\n",
        "stop_words='english', #영어불용어\n",
        "ngram_range = (1,1), #1-gram\n",
        "tokenizer = token.tokenize) #알파벳, 숫자만 잘라서 인코딩\n",
        "text_counts= cv3.fit_transform(text)\n",
        "df_x4 = pd.DataFrame(text_counts.toarray(), columns=cv3.get_feature_names())\n",
        "df_x4\n",
        "\n",
        "# 훈련데이터 읽기 (PhraseId를 인덱스로!)\n",
        "sent = pd.read_csv('/content/sent_train.tsv', sep='\\t')\n",
        "sent.set_index('PhraseId', inplace=True)\n",
        "sent\n",
        "\n",
        "# 데이터프레임 요약 head(), shape, dtypes, isna().sum(), describe()\n",
        "sent.head() #상위 5개 데이터 검색\n",
        "\n",
        "sent.shape #156060개의 레코드\n",
        "\n",
        "sent.dtypes #Sentiment가 범주형 데이터 임\n",
        "\n",
        "sent.isna().sum() #널값 체크\n",
        "\n",
        "sent.describe() #기초 통계\n",
        "\n",
        "\"\"\"# 데이터시각화\"\"\"\n",
        "\n",
        "# 박스플랏과 히스토그램을 이용한 일변량분석\n",
        "sent.boxplot('Sentiment')\n",
        "plt.show()\n",
        "\n",
        "sent.hist('Sentiment')\n",
        "plt.show()\n",
        "\n",
        "# 감성점수에 따른 데이터 갯수\n",
        "sent.columns\n",
        "tab = sent.groupby('Sentiment').size()\n",
        "tab\n",
        "\n",
        "# 감정점수에 따른 막대그래프\n",
        "sns.countplot(sent.Sentiment)\n",
        "plt.xticks([0,1,2,3,4],['negative', 'somewhat negative', 'neutral', 'somewhat positive', 'positive'], rotation=90)\n",
        "plt.show()\n",
        "\n",
        "\"\"\"# 전처리\"\"\"\n",
        "\n",
        "import nltk\n",
        "nltk.download('popular')\n",
        "\n",
        "# 불용어 및 문장기호 제거\n",
        "import string\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "ps = PorterStemmer()\n",
        "\n",
        "def clean_text(text):\n",
        "    text = \"\".join([char.lower() for char in text if char not in string.punctuation])\n",
        "    tokens = re.split('\\W+', text) #or word_tokenize(text)\n",
        "    text = [ps.stem(word) for word in tokens if word not in set(stopwords.words('english'))]\n",
        "    return text\n",
        "\n",
        "# 너무 오래 걸려서 사이즈를 줄임\n",
        "sent = sent[:1000]\n",
        "sent.shape\n",
        "\n",
        "lst = []\n",
        "for p in sent.Phrase: #줄별로 실행\n",
        "    lst.append(' '.join(clean_text(p))) #전처리후 줄별로 합침\n",
        "    print('{} added'.format(p))\n",
        "sent.Phrase = lst #전처리한 데이터를 구문컬럼에 저장\n",
        "sent.to_csv('sent.tsv', sep='\\t')\n",
        "\n",
        "# 독립, 종속변수 분리\n",
        "y = sent.Sentiment\n",
        "x = sent.Phrase\n",
        "\n",
        "# 단어 벡터화\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "\n",
        "token = RegexpTokenizer(r'[a-zA-Z0-9]+') #글자, 숫자이외의 것은 제거\n",
        "cv = CountVectorizer(lowercase=True, #소문자\n",
        "                     stop_words='english', #불용어\n",
        "                     ngram_range = (1,1), #1 gram\n",
        "                     tokenizer = token.tokenize) #정규식을 이용해서 자름\n",
        "df_x = pd.DataFrame(cv.fit_transform(x).toarray(), columns=cv.get_feature_names())\n",
        "df_x.head()\n",
        "\n",
        "# 훈련, 테스트 데이터 분리 (sklearn.model_selection, test=.30, random_state))\n",
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_test, y_train, y_test = train_test_split(df_x, y, test_size=.30, random_state=1)\n",
        "x_train.shape, x_test.shape, y_train.shape, y_test.shape\n",
        "\n",
        "# 로지스틱 회귀분석\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "model = LogisticRegression() #모델 생성\n",
        "model.fit(x_train, y_train) #모델 훈련\n",
        "model.score(x_test, y_test) #정확도를 이용해서 평가\n",
        "\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "y_pred = model.predict(x_test) #테스트 데이터의 예측값\n",
        "sns.heatmap(confusion_matrix(y_test, y_pred), annot=True, fmt='g') #혼동행렬\n",
        "plt.show()\n",
        "\n",
        "# 분류보고서\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "# 머신러닝 모델 라이브러리 가져오기\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "\n",
        "models = [DecisionTreeClassifier(), RandomForestClassifier(), KNeighborsClassifier(), SVC()]\n",
        "names = ['DT', 'RF', 'KNN', 'SVM']\n",
        "for model, name in zip(models, names):\n",
        "    model.fit(x_train,y_train) #훈련\n",
        "    acc_score = model.score(x_test, y_test) #평가\n",
        "    y_pred = model.predict(x_test) #예측\n",
        "    sns.heatmap(confusion_matrix(y_test, y_pred), annot=True, fmt='g') #혼동행렬\n",
        "    plt.title('{}: {}'.format(name, acc_score))\n",
        "    plt.show()\n",
        "    print(classification_report(y_test, y_pred)) #분류보고서\n",
        "\n",
        "# 테스트 데이터로 예측\n",
        "text = 'So there is no way for me to plug it in here in the US unless I go by a converter.'\n",
        "text = ' '.join(clean_text(text))\n",
        "test_data = pd.DataFrame({'Phrase':text},index=[0])\n",
        "df_test = pd.DataFrame(cv.transform(test_data).toarray(), columns=cv.get_feature_names())\n",
        "print(models[1].predict(df_test))\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#COUNT VECTERIZER\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "corpus = ['This is the first document.',\n",
        "          'This document is the second document.',\n",
        "          'And this is the third one.',\n",
        "          'Is this the first document?']\n",
        "vectorizer = CountVectorizer()\n",
        "X = vectorizer.fit_transform(corpus)\n",
        "print(vectorizer.get_feature_names())\n",
        "print(X.toarray())\n",
        "X_df = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names())\n",
        "a = X_df.head(10)\n",
        "\n",
        "#ngram\n",
        "vectorizer2 = CountVectorizer(analyzer='word', ngram_range=(2, 2)) #word n-gram\n",
        "X2 = vectorizer2.fit_transform(corpus)\n",
        "print(vectorizer2.get_feature_names())\n",
        "print(X2.toarray())\n",
        "X2_df = pd.DataFrame(X2.toarray(), columns=vectorizer2.get_feature_names())\n",
        "b = X2_df.head(10)\n",
        "\n",
        "#tfidf\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "vectorizer3 = TfidfVectorizer()\n",
        "X3 = vectorizer3.fit_transform(corpus)\n",
        "print(vectorizer3.get_feature_names())\n",
        "print(X3.toarray())\n",
        "X3_df = pd.DataFrame(X3.toarray(), columns=vectorizer3.get_feature_names())\n",
        "c = X3_df.head(10)\n",
        "\n",
        "#RegexpTokenizer\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "sentence  = \"Think and wonder, wonder and think.\"\n",
        "token = RegexpTokenizer(r\"\\w+\")\n",
        "new_words = token.tokenize(sentence)\n",
        "print(new_words)\n",
        "\n",
        "#posterior probability\n",
        "weather = ['sunny', 'overcast', 'rainy', 'sunny', 'sunny', 'overcast', 'rainy', 'rainy', 'sunny', 'rainy', 'sunny', 'overcast', 'overcast', 'rainy']\n",
        "play = ['no', 'yes', 'yes', 'yes', 'yes', 'yes', 'no', 'no', 'yes', 'yes', 'no', 'yes', 'yes', 'no']\n",
        "df = pd.DataFrame({'weather':weather,\n",
        "                   'play':play})\n",
        "tab = pd.crosstab(df.weather, df.play, margins=True, margins_name='total')\n",
        "tab\n",
        "overcast = tab.total[0]\n",
        "rainy = tab.total[1]\n",
        "sunny = tab.total[2]\n",
        "no = tab.iloc[3,0]\n",
        "yes = tab.iloc[3,1]\n",
        "p_yes_sunny = (len(df[(df.weather == 'sunny')&(df.play == 'yes')])/yes)*yes/sunny\n",
        "\n",
        "#text classification\n",
        "p1 = (2+1)/(11+14) * (1+1)/(11+14) * (0+1)/(11+14) * (2+1)/(11+14) * (3/5)\n",
        "p2 = (1+1)/(9+14) * (0+1)/(9+14) * (1+1)/(9+14) * (0+1)/(9+14) * (2/5)\n",
        "\n",
        "#Naive Bayesian Model Example\n",
        "tennis = pd.read_csv('PlayTennis.csv')\n",
        "df = tennis.copy()\n",
        "\n",
        "#encoding\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "le = LabelEncoder()\n",
        "col_names = list(df.columns)\n",
        "#col_names = []\n",
        "#for i in df.columns:\n",
        "#    col_names.append(i)\n",
        "for i in col_names:\n",
        "    df[i] = le.fit_transform(df[i])\n",
        "\n",
        "#x and y split\n",
        "df.columns\n",
        "y = df['Play Tennis']\n",
        "x = df.drop('Play Tennis', axis=1)\n",
        "\n",
        "#train and test split\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB, BernoulliNB, MultinomialNB\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(x,y,test_size=.3, random_state=1)\n",
        "\n",
        "#MODELS\n",
        "gnb = GaussianNB()\n",
        "gnb.fit(x_train, y_train)\n",
        "y_pred = gnb.predict(x_test)\n",
        "gnb_score = accuracy_score(y_test, y_pred) #.8\n",
        "# confusion_matrix(y_test, y_pred)\n",
        "# print(classification_report(y_test, y_pred))\n",
        "\n",
        "bnb = BernoulliNB() #binary\n",
        "bnb.fit(x_train, y_train)\n",
        "y_pred = bnb.predict(x_test)\n",
        "bnb_score = accuracy_score(y_test, y_pred) #.6\n",
        "\n",
        "mnb = MultinomialNB() #범주형\n",
        "mnb.fit(x_train, y_train)\n",
        "y_pred = mnb.predict(x_test)\n",
        "mnb_score = accuracy_score(y_test, y_pred) #.6\n",
        "\n",
        "print(gnb_score, bnb_score, mnb_score)\n",
        "\n",
        "test_data = pd.DataFrame([[1, 2, 0, 0]], columns=col_names[:4])\n",
        "gnb.predict(test_data) #0\n",
        "mnb.predict(test_data) #0\n",
        "bnb.predict(test_data) #0\n",
        "\n",
        "#fruit prediction problem\n",
        "fruit = pd.read_csv('fruit_data_with_colors.txt', sep='\\t')\n",
        "fruit.head()\n",
        "fruit.columns\n",
        "\n",
        "#x and y split\n",
        "y = fruit.fruit_label\n",
        "x = fruit.drop(['fruit_label', 'fruit_name', 'fruit_subtype'], axis=1)\n",
        "\n",
        "x.boxplot()\n",
        "plt.show()\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "scaler = MinMaxScaler()\n",
        "x_scaled = scaler.fit_transform(x)\n",
        "x_df = pd.DataFrame(x_scaled, columns=x.columns)\n",
        "x_df.boxplot()\n",
        "\n",
        "#train and test split\n",
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_test, y_train, y_test = train_test_split(x,y, test_size=.2, random_state=1)\n",
        "\n",
        "#bayesian!!!!! 다같이 .6\n",
        "from sklearn.naive_bayes import GaussianNB, MultinomialNB, BernoulliNB\n",
        "from sklearn.metrics import accuracy_score\n",
        "gnb = GaussianNB()\n",
        "gnb.fit(x_train, y_train) #***\n",
        "y_pred = gnb.predict(x_test)\n",
        "gnb_score = accuracy_score(y_test, y_pred) #*****\n",
        "\n",
        "mnb = MultinomialNB()\n",
        "mnb.fit(x_train, y_train)\n",
        "y_pred = mnb.predict(x_test)\n",
        "mnb_score = accuracy_score(y_test, y_pred)\n",
        "\n",
        "bnb = BernoulliNB()\n",
        "bnb.fit(x_train, y_train)\n",
        "y_pred = bnb.predict(x_test)\n",
        "bnb_score = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(gnb_score, mnb_score, bnb_score)\n",
        "\n",
        "test_data = x_train.iloc[[0]]\n",
        "y_train.iloc[[0]]\n",
        "# test_data = scaler.transform(test_data)\n",
        "# test_data = pd.DataFrame(test_data, columns=x.columns)\n",
        "gnb.predict(test_data)\n",
        "\n",
        "#phrase and Sentiment Analysis\n",
        "# Read sent_train.tsv 훈련데이터 읽기\n",
        "data=pd.read_csv('sent_train.tsv', sep='\\t')\n",
        "data.head()\n",
        "data.dtypes\n",
        "\n",
        "# Count phrase by sentiments and draw bar chart. 감정에 따른 표현을 카운트, 막대그래프\n",
        "data.columns\n",
        "tab = data.Sentiment.value_counts(sort=False)\n",
        "# tab =data.groupby('Sentiment').count()\n",
        "plt.style.use('ggplot')\n",
        "plt.bar(tab.index, tab)\n",
        "plt.xlabel('Review Sentiments')\n",
        "plt.ylabel('Number of Review')\n",
        "plt.show()\n",
        "\n",
        "#import string\n",
        "#import re\n",
        "#from nltk.corpus import stopwords\n",
        "#from nltk.stem import PorterStemmer\n",
        "#ps = PorterStemmer()\n",
        "#\n",
        "#def clean_text(text):\n",
        "#    text = \"\".join([word.lower() for word in text if word not in string.punctuation])\n",
        "#    tokens = re.split('\\W+', text) #or word_tokenize(text)\n",
        "#    text = [ps.stem(word) for word in tokens if word not in set(stopwords.words('english'))]\n",
        "#    return text\n",
        "#clean_text(text)\n",
        "\n",
        "# Remove stopwords and punctuations. 스탑워드와 문장기호 삭제\n",
        "# Use CountVectorizer and TF-IDF to generate bag of words. 단어를 벡터화\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "\n",
        "#tokenizer to remove unwanted elements from out data like symbols and numbers\n",
        "token = RegexpTokenizer(r'[a-zA-Z0-9]+')\n",
        "cv = CountVectorizer(lowercase=True,stop_words='english',ngram_range = (1,1),tokenizer = token.tokenize)\n",
        "phrase_encoded = cv.fit_transform(data.Phrase)\n",
        "\n",
        "# Train and test split (.3 test set). 30프로의 테스트셋으로 분리\n",
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_test, y_train, y_test = train_test_split(phrase_encoded, data.Sentiment, test_size=0.3, random_state=1)\n",
        "\n",
        "# Build and evaluate a naïve Bayesian model (BernoulliNB, MultinomialNB). 나이브베이즈 모델 생성\n",
        "from sklearn.naive_bayes import MultinomialNB #범주형\n",
        "# Model Generation Using Multinomial Naive Bayes\n",
        "mnb = MultinomialNB()\n",
        "mnb.fit(x_train, y_train)\n",
        "y_pred = mnb.predict(X_test)\n",
        "mnb_score = accuracy_score(y_test, y_pred)\n",
        "\n",
        "#evaluating output\n",
        "#from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "#print(\"MultinomialNB Test Accuracy:\", accuracy_score(y_test, y_pred)) #test performance 0.6049169122986885\n",
        "#print(\"MultinomialNB Training Accuracy:\", accuracy_score(y_train, mnb.predict(x_train))) #train performance\n",
        "#confusion_matrix(y_test, y_pred) #test performance\n",
        "#print(classification_report(y_test, y_pred))\n",
        "\n",
        "# gnb = GaussianNB()\n",
        "# gnb.fit(x_train.toarray(), y_train) #***\n",
        "# y_pred = gnb.predict(x_test)\n",
        "# gnb_score = accuracy_score(y_test, y_pred) #*****\n",
        "\n",
        "bnb = BernoulliNB()\n",
        "bnb.fit(x_train, y_train)\n",
        "y_pred = bnb.predict(x_test)\n",
        "bnb_score = accuracy_score(y_test, y_pred) #0.6043615703361955\n",
        "\n",
        "# Compare the results. 결과 비교\n",
        "print(mnb_score, bnb_score) #mnb\n",
        "\n",
        "# Prediction with test data. 테스트데이터로 예측\n",
        "test_data = pd.DataFrame({'Phrase':'hello my name is kim'}, index=[0])\n",
        "test_data_num= cv.transform(test_data)\n",
        "test_data_num.toarray()\n",
        "mnb.predict(test_data_num)"
      ]
    }
  ]
}
